{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP3回目\n",
        "\n",
        "3回目はボリュームが多いので、notebookを実行しながら理解してもらう形式にします"
      ],
      "metadata": {
        "id": "JxIbvrHU_oVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformerのtokenizerを使って学習を行う\n",
        "\n",
        "(正確にはファインチューニング)"
      ],
      "metadata": {
        "id": "Knee5mp4hsOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformersのライブラリのインストール"
      ],
      "metadata": {
        "id": "B_lME3Ks0FHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "cLm0ZGz_QOq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# トークナイザーの実践例"
      ],
      "metadata": {
        "id": "bW69818NRWHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "トークナイザーの指定と特定の事前学習済みモデルの重みの読み込み"
      ],
      "metadata": {
        "id": "hrvw5QcgRLST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# トークナイザーとそのトークナイザーで学習したチェックポイントの重みの読み込み\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "Q9nA2-NqQ8bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "トークン化(tokenize)"
      ],
      "metadata": {
        "id": "LHWDmGkOSMf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_en = tokenizer.tokenize(\"I have a pen\")\n",
        "tokenized_en"
      ],
      "metadata": {
        "id": "4QX-t0zRSWAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_en2 = tokenizer.tokenize(\"The AI technology is evolving unpredictably.\")\n",
        "tokenized_en2"
      ],
      "metadata": {
        "id": "Cc3DsgUJbXK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ja = tokenizer.tokenize(\"僕はペンを持っている。\")\n",
        "tokenized_ja"
      ],
      "metadata": {
        "id": "fBvDI8RZSsFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "エンコーディング(encode)"
      ],
      "metadata": {
        "id": "SO25bMniSJVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#引数は文字列\n",
        "encoded_en = tokenizer.encode(\"I have a pen\")\n",
        "print(encoded_en)\n",
        "type(encoded_en)"
      ],
      "metadata": {
        "id": "8QmcSYVwS75-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "デコーディング(decode)"
      ],
      "metadata": {
        "id": "0-JjNlEMVG2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer.decode()はencodeしたデータを自然言語に戻す.  \n",
        "引数はnumpy配列なので変換が必要"
      ],
      "metadata": {
        "id": "wEekdU3eWHdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "encoded_en_numpy = np.array(encoded_en)\n",
        "print(encoded_en_numpy)\n",
        "type(encoded_en_numpy)"
      ],
      "metadata": {
        "id": "YGKg2yRIWX_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#引数はnumpy配列\n",
        "decoded_en = tokenizer.decode(encoded_en_numpy)\n",
        "print(decoded_en)"
      ],
      "metadata": {
        "id": "vXyU7UxpTuFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑   \n",
        "[CLS]は文の始まりを意味しており、このトークナイザーでは101.  \n",
        "[SEP]は文の終わりを意味しており、このトークナイザーでは102.   \n",
        "[UNK]は認識できない単語(未知語).   \n",
        "そのままでは日本語がうまくいかない\n"
      ],
      "metadata": {
        "id": "2yT03gIweC0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fugashiは形態素解析ツール unidic_liteは辞書\n",
        "!pip install fugashi\n",
        "!pip install unidic_lite\n",
        "from transformers import BertJapaneseTokenizer\n",
        "tokenizer2 = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')\n"
      ],
      "metadata": {
        "id": "haz6KgzEiZD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ja2 = tokenizer2.tokenize(\"僕はペンを持っている。\")\n",
        "tokenized_ja2"
      ],
      "metadata": {
        "id": "-_VdrMFaiZBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_ja2 = tokenizer2.encode(\"僕はペンを持っている。\")\n",
        "print(encoded_ja2)"
      ],
      "metadata": {
        "id": "5pQgTRsXeCRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_ja_numpy2 = np.array(encoded_ja2)\n",
        "print(encoded_ja_numpy2)"
      ],
      "metadata": {
        "id": "DWu6SQFKu0ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_ja2 = tokenizer2.decode(encoded_ja_numpy2)\n",
        "print(decoded_ja2)"
      ],
      "metadata": {
        "id": "_Wfsj1gfu20m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 複数の行をencodeさせるにはbatch_encode_plusを使用する.  \n"
      ],
      "metadata": {
        "id": "3mVBuAWBnu0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "token_type_idsは文ごとにIDを振りたい時に用いる(デフォルトは0)  \n",
        "attention_maskはbertモデルに単語(トークン)と認識させるかどうか(トークンであれば1)"
      ],
      "metadata": {
        "id": "GFXoI5MnvHl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_temp = [\"僕はペンを持っている。\",\"彼らは鉛筆を探しているが、見つからない。\"]\n",
        "temp2 = tokenizer2.batch_encode_plus(text_temp)\n",
        "temp2"
      ],
      "metadata": {
        "id": "VeLBez51oEon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "temp2を見やすく"
      ],
      "metadata": {
        "id": "SIkx3_Z5uBwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "  print(f\"{i+1}文目\")\n",
        "  print(temp2['input_ids'][i])\n",
        "  print(temp2['attention_mask'][i])\n",
        "  print(f\"トークン長:{len(temp2['input_ids'][i])}\")\n"
      ],
      "metadata": {
        "id": "vy_-MQTouEME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑.  \n",
        "順に処理されて'input_ids'、'token_type_ids'、'attention_mask'に2つずつデータが格納されている.  \n",
        "token_type_idsは全て0.  \n",
        "attention_maskは全て1(=全てトークン).   \n",
        "トークンの長さは文の長い2つ目の方が多くなっている"
      ],
      "metadata": {
        "id": "AYTCeKEcs9-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**全ての文のサイズを同じにする**.  \n",
        "実際には複数の文を全て同じサイズにする.     \n",
        "画像処理で行なったMLPやCNNも全て同じサイズにしていた.  \n",
        "するとバッチサイズごとに行列演算などが可能になる.   "
      ],
      "metadata": {
        "id": "ngZz5GKLtrLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "オプション(引数)を追加する.  \n",
        "     "
      ],
      "metadata": {
        "id": "NxrGzlscvME4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_length：不足部分を補うトークンの最大長.  \n",
        "padding：'max_length'にするとmax_length(下の場合14)のトークン長に合わせて不足部分を0で埋める"
      ],
      "metadata": {
        "id": "6DbnOabIxTDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp3 = tokenizer2.batch_encode_plus(text_temp,\n",
        "                                       max_length=14,\n",
        "                                       padding='max_length',\n",
        "                                       )\n",
        "print(temp3)\n",
        "for i in range(2):\n",
        "  print(f\"{i+1}文目\")\n",
        "  print(temp3['input_ids'][i])\n",
        "  print(temp3['attention_mask'][i])\n",
        "  print(f\"トークン長:{len(temp3['input_ids'][i])}\")"
      ],
      "metadata": {
        "id": "kA7YNULHxSWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "truncation：余剰部分もmax_lengthの長さを揃える"
      ],
      "metadata": {
        "id": "FwjA-jHXyPNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp3 = tokenizer2.batch_encode_plus(text_temp,\n",
        "                                       max_length=14,\n",
        "                                       padding='max_length',\n",
        "                                       truncation=True,\n",
        "                                       )\n",
        "print(temp3)\n",
        "for i in range(2):\n",
        "  print(f\"{i+1}文目\")\n",
        "  print(temp3['input_ids'][i])\n",
        "  print(temp3['attention_mask'][i])\n",
        "  print(f\"トークン長:{len(temp3['input_ids'][i])}\")"
      ],
      "metadata": {
        "id": "6HSAlfBdwrec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "return_token_type_ids：Falseにするとtoken_type_idが出力されない.  \n",
        "return_tensors：出力する配列データの型.    \n",
        "tf：tensorflow.    \n",
        "pt：pytorch.     \n",
        "np：numpy."
      ],
      "metadata": {
        "id": "xJ8KUQjZ23JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp3 = tokenizer2.batch_encode_plus(text_temp,\n",
        "                                       max_length=14,\n",
        "                                       padding='max_length',\n",
        "                                       truncation=True,\n",
        "                                       return_token_type_ids=False,\n",
        "                                       return_tensors='np'\n",
        "                                       )\n",
        "temp3"
      ],
      "metadata": {
        "id": "R0LPhMBQzXGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それぞれ抽出"
      ],
      "metadata": {
        "id": "viKDaf9CtO2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp3['input_ids'])\n",
        "print(temp3['attention_mask'])"
      ],
      "metadata": {
        "id": "lgNeQ_9vyEjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "57vZKUue3PMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  扱うデータを読み込む.  \n",
        "今回はChatGPTに作らせた架空のデータを用意.  \n",
        "がんを疑う所見か否か"
      ],
      "metadata": {
        "id": "wZoAPfykh4ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# CSVファイルの読み込み\n",
        "df = pd.read_csv('/content/drive/MyDrive/shuffled_medical_texts_labels.csv')"
      ],
      "metadata": {
        "id": "yYYZtlfZkIRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BkCwxjFxfZBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "中身を確認"
      ],
      "metadata": {
        "id": "xwD4cFDGkltb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "L9JzZmGDknm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データを訓練用とテスト用に分割"
      ],
      "metadata": {
        "id": "q5EfKgG-kyge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# テキストとラベルを分離\n",
        "texts = df['texts'].tolist()\n",
        "labels = df['labels'].tolist()\n",
        "\n",
        "# データを訓練用とテスト用に分割\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)"
      ],
      "metadata": {
        "id": "8a2OCqNJk27U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "中身の確認"
      ],
      "metadata": {
        "id": "WKgOtxgVlE66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_texts))\n",
        "print(len(test_texts))\n",
        "print(len(train_labels))\n",
        "print(len(test_labels))"
      ],
      "metadata": {
        "id": "q-fUB5wDloFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# エンコード\n",
        "train_encodings = tokenizer2.batch_encode_plus(train_texts,\n",
        "                                             max_length=512,\n",
        "                                             padding='max_length',\n",
        "                                              truncation=True,\n",
        "                                              return_token_type_ids=False,\n",
        "                                              return_tensors='tf')\n",
        "train_encodings"
      ],
      "metadata": {
        "id": "SM9zMTiC4XZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# エンコード\n",
        "test_encodings = tokenizer2.batch_encode_plus(test_texts,\n",
        "                                             max_length=512,\n",
        "                                             padding='max_length',\n",
        "                                              truncation=True,\n",
        "                                              return_token_type_ids=False,\n",
        "                                              return_tensors='tf')\n",
        "test_encodings"
      ],
      "metadata": {
        "id": "k-r-OjewTWOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデルの作成"
      ],
      "metadata": {
        "id": "4FXCJc8A6IUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from transformers import TFBertModel\n",
        "\n",
        "# モデルの初期化\n",
        "bert_model = TFBertModel.from_pretrained('cl-tohoku/bert-base-japanese-v3')\n",
        "\n",
        "# BERTの入力\n",
        "input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "# BERTの出力\n",
        "bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
        "\n",
        "# 分類のための層を追加\n",
        "clf_output = bert_output[:, 0, :]\n",
        "out = Dense(1, activation='sigmoid')(clf_output)\n",
        "\n",
        "# モデルの定義\n",
        "model2 = Model(inputs=[input_ids, attention_mask], outputs=out)\n",
        "\n",
        "# モデルのコンパイル\n",
        "model2.compile(optimizer=Adam(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "I4PhebF0W6_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "テストデータの中身の確認"
      ],
      "metadata": {
        "id": "WU4NCo2NkQo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# zip(リスト1,リスト2)とすることで同時にfor文で処理できる\n",
        "for i,j in zip(test_texts,test_labels):\n",
        "  print(i,j)"
      ],
      "metadata": {
        "id": "s_T0eFw26qVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習前後で結果を比較するための学習前の予測"
      ],
      "metadata": {
        "id": "zHcQHhW8myB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルに入力し予測\n",
        "predictions_pre = model2.predict([test_encodings['input_ids'], test_encodings['attention_mask']])\n",
        "\n",
        "# 予測結果の表示\n",
        "print(predictions_pre)"
      ],
      "metadata": {
        "id": "2yauaxFc6qTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j,k in zip(test_texts,test_labels,predictions_pre):\n",
        "  print(i,j,k)"
      ],
      "metadata": {
        "id": "PpEAxswu8X3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 学習"
      ],
      "metadata": {
        "id": "snVjPsBt7syf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習\n",
        "results = model2.fit({'input_ids': train_encodings['input_ids'],\n",
        "           'attention_mask': train_encodings['attention_mask']},\n",
        "          tf.convert_to_tensor(train_labels),\n",
        "          validation_data=({'input_ids': test_encodings['input_ids'][2:],\n",
        "                            'attention_mask': test_encodings['attention_mask'][2:]},\n",
        "                           tf.convert_to_tensor(test_labels[2:])),\n",
        "          epochs=3,\n",
        "          batch_size=8)"
      ],
      "metadata": {
        "id": "e9zuKu0e7172"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの予測\n",
        "predictions = model2.predict([test_encodings['input_ids'], test_encodings['attention_mask']])\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "bKloBRM98PGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後の正解,学習前、学習後\n",
        "for i,j,k,l in zip(test_texts,test_labels,predictions_pre,predictions):\n",
        "  print(i,j,\"学習前の予測\",k,\"→\",\"学習後の予測\",l)"
      ],
      "metadata": {
        "id": "v1acoRMW6HWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価(おまけ)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from sklearn.metrics import precision_score, f1_score\n",
        "\n",
        "# モデルの予測\n",
        "predictions_round = np.round(predictions).flatten()  # 確率を0または1に丸める\n",
        "print(f\"19件の予測結果:{predictions_round}\")\n",
        "# 精度とリコールの計算\n",
        "accuracy = accuracy_score(test_labels, predictions_round)\n",
        "recall = recall_score(test_labels, predictions_round)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# 精度とF1スコアの計算\n",
        "precision = precision_score(test_labels, predictions_round)\n",
        "f1 = f1_score(test_labels, predictions_round)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "AanKF76v7W42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVOBPaxs7W2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YQUsMr35-w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYNDZgjg5-rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TU1O1dT05-o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbhyYOqXfdow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB1xLMPmz-yt"
      },
      "outputs": [],
      "source": []
    }
  ]
}